---
title: "Evaluations"
description: "Score agent responses with automated scorers and build quality scorecards."
section: "agents"
order: 10
primaryKeyword: "AI agent evaluation"
relatedSlugs: ["agents/guardrails", "agents/learning"]
pageType: "how-to"
---

# Evaluations

Evaluations are automated quality scores computed after each agent run. They measure response quality, safety, relevance, and other dimensions using pre-built scorers from Mastra's evaluation framework.

## What are evaluations?

Evaluations are post-run quality assessments that score agent outputs across multiple dimensions. Each evaluation runs one or more **scorers**—specialized functions that measure specific aspects of agent performance.

Evaluations are:

- **Automatic** — Run after every completed run (if scorers are configured)
- **Asynchronous** — Don't block the user response
- **Stored** — Persisted in `AgentEvaluation` for historical analysis
- **Configurable** — Choose which scorers to run per agent

<Callout type="info">
    Evaluations are **non-blocking**—they run after the response is sent to the user. This ensures
    low latency while still providing quality metrics.
</Callout>

## Scorer types

AgentC2 includes several pre-built scorers:

| Scorer ID       | Name                    | Description                               | Scale   | Higher is Better        |
| --------------- | ----------------------- | ----------------------------------------- | ------- | ----------------------- |
| `relevancy`     | Answer Relevancy        | How well the response addresses the query | 0.0-1.0 | ✅ Yes                  |
| `toxicity`      | Toxicity                | Harmful content detection                 | 0.0-1.0 | ❌ No (lower is better) |
| `completeness`  | Completeness            | Information coverage and thoroughness     | 0.0-1.0 | ✅ Yes                  |
| `helpfulness`   | Helpfulness             | How useful the response is to the user    | 0.0-1.0 | ✅ Yes                  |
| `hallucination` | Hallucination Detection | Factual accuracy, made-up information     | 0.0-1.0 | ❌ No (lower is better) |
| `tone`          | Tone Consistency        | Style and formality consistency           | 0.0-1.0 | ✅ Yes                  |

### Relevancy scorer

Measures how well the response addresses the user's question:

```json
{
    "scorer": "relevancy",
    "score": 0.87,
    "reason": "Response directly addresses the question about AI frameworks and provides specific examples"
}
```

Use for: Ensuring agents stay on topic and answer the actual question.

### Toxicity scorer

Detects harmful, offensive, or inappropriate content:

```json
{
    "scorer": "toxicity",
    "score": 0.05, // Low score = safe content
    "reason": "No harmful content detected"
}
```

<Callout type="warning">
    Toxicity scores are **inverted**—lower scores indicate safer content. A score of 0.0 means no
    toxicity detected, while 1.0 indicates highly toxic content.
</Callout>

### Completeness scorer

Assesses whether the response covers all aspects of the question:

```json
{
    "scorer": "completeness",
    "score": 0.72,
    "reason": "Response covers main points but lacks detail on implementation"
}
```

Use for: Ensuring agents provide thorough, comprehensive answers.

### Helpfulness scorer

Measures how useful the response is to the user:

```json
{
    "scorer": "helpfulness",
    "score": 0.91,
    "reason": "Response provides actionable steps and concrete examples"
}
```

Use for: Ensuring agents deliver value, not just information.

### Hallucination scorer

Detects when agents make up facts or information:

```json
{
    "scorer": "hallucination",
    "score": 0.12, // Low score = factual
    "reason": "Claims are supported by tool results and citations"
}
```

<Callout type="tip">
    Hallucination detection works best when agents use tools and cite sources. Encourage tool usage
    in instructions to improve hallucination scores.
</Callout>

### Tone scorer

Measures consistency in style and formality:

```json
{
    "scorer": "tone",
    "score": 0.85,
    "reason": "Consistent professional tone throughout the response"
}
```

Use for: Ensuring brand consistency and appropriate communication style.

## Configuring scorers on agents

Set which scorers to run via the `scorers` field:

### During agent creation

```bash
curl -X POST https://agentc2.ai/agent/api/agents \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Research Assistant",
    "slug": "research-assistant",
    "instructions": "You are a research assistant...",
    "modelProvider": "openai",
    "modelName": "gpt-4o",
    "scorers": ["relevancy", "completeness", "hallucination"]
  }'
```

### Updating existing agents

```bash
curl -X PATCH https://agentc2.ai/agent/api/agents/research-assistant \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "scorers": ["relevancy", "completeness", "helpfulness", "hallucination"]
  }'
```

<Callout type="info">
    Scorers are specified as a **string array** of scorer IDs. The order doesn't matter—all
    configured scorers run in parallel.
</Callout>

## Evaluation results

Evaluations are stored per run in the `AgentEvaluation` model:

| Field        | Type     | Description                      |
| ------------ | -------- | -------------------------------- |
| `id`         | String   | Unique identifier (CUID)         |
| `runId`      | String   | Foreign key to `AgentRun`        |
| `agentId`    | String   | Foreign key to `Agent`           |
| `scoresJson` | JSON     | Map of scorer ID to score value  |
| `createdAt`  | DateTime | When the evaluation was computed |

Query evaluation results:

```bash
curl "https://agentc2.ai/agent/api/agents/research-assistant/runs?includeEvaluations=true&limit=10" \
  -H "Authorization: Bearer $TOKEN"
```

Response includes scores:

```json
{
    "runs": [
        {
            "id": "run-123",
            "inputText": "What are the latest AI developments?",
            "outputText": "Recent developments include...",
            "evaluation": {
                "scoresJson": {
                    "relevancy": 0.89,
                    "completeness": 0.76,
                    "helpfulness": 0.82,
                    "hallucination": 0.08
                }
            }
        }
    ]
}
```

## Scorecard visualization

Aggregate scores across runs to build quality scorecards:

```bash
curl "https://agentc2.ai/agent/api/agents/research-assistant/evaluations?aggregate=true&startDate=2026-02-01" \
  -H "Authorization: Bearer $TOKEN"
```

Response includes aggregated metrics:

```json
{
    "summary": {
        "totalRuns": 150,
        "runsEvaluated": 148,
        "avgScores": {
            "relevancy": 0.87,
            "completeness": 0.74,
            "helpfulness": 0.81,
            "hallucination": 0.12
        },
        "scoreDistribution": {
            "relevancy": {
                "excellent": 45, // > 0.9
                "good": 78, // 0.7-0.9
                "fair": 20, // 0.5-0.7
                "poor": 5 // < 0.5
            }
        }
    },
    "trends": {
        "relevancy": {
            "trend": "improving",
            "change": 0.05 // +5% over last period
        }
    }
}
```

Use scorecards to:

- **Identify quality issues** — Low scores indicate problems
- **Track improvements** — Monitor scores over time
- **Compare versions** — A/B test different configurations
- **Set quality thresholds** — Define minimum acceptable scores

## Continuous evaluation vs batch evaluation

### Continuous evaluation (default)

Scorers run automatically after every run:

```json
{
    "scorers": ["relevancy", "completeness"]
}
```

**Pros:**

- Real-time quality monitoring
- Immediate feedback on changes
- No manual intervention needed

**Cons:**

- Higher compute costs (every run is evaluated)
- May slow down high-volume agents

### Batch evaluation

Run evaluations on-demand for specific runs:

```bash
curl -X POST https://agentc2.ai/agent/api/agents/research-assistant/evaluations \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "runIds": ["run-123", "run-124", "run-125"],
    "scorers": ["relevancy", "completeness", "helpfulness"]
  }'
```

**Pros:**

- Lower compute costs
- Evaluate only important runs
- Test new scorers without affecting production

**Cons:**

- Manual process
- Delayed feedback

<Callout type="tip">
    Start with **continuous evaluation** for new agents to establish baselines. Switch to **batch
    evaluation** for high-volume agents where cost is a concern.
</Callout>

## Using evaluations to guide improvements

### 1. Identify low-scoring runs

Find runs that need improvement:

```bash
curl "https://agentc2.ai/agent/api/agents/research-assistant/runs?minScore=0.5&scorer=relevancy&limit=20" \
  -H "Authorization: Bearer $TOKEN"
```

Review these runs to identify patterns:

- Common failure modes
- Instruction ambiguities
- Tool usage issues

### 2. Compare versions

Evaluate different agent configurations:

```bash
# Compare version 2 vs version 3
curl "https://agentc2.ai/agent/api/agents/research-assistant/evaluations?versionId=version-122&compareWith=version-123" \
  -H "Authorization: Bearer $TOKEN"
```

Response shows which version performs better:

```json
{
    "versionA": {
        "version": 2,
        "avgScores": {
            "relevancy": 0.82,
            "completeness": 0.71
        }
    },
    "versionB": {
        "version": 3,
        "avgScores": {
            "relevancy": 0.89,
            "completeness": 0.78
        }
    },
    "improvement": {
        "relevancy": 0.07, // +7%
        "completeness": 0.07 // +7%
    }
}
```

### 3. Set quality thresholds

Define minimum acceptable scores and alert on violations:

```bash
# Find runs below threshold
curl "https://agentc2.ai/agent/api/agents/research-assistant/runs?maxScore=0.6&scorer=relevancy" \
  -H "Authorization: Bearer $TOKEN"
```

Set up monitoring to alert when scores drop below thresholds.

### 4. Iterate on instructions

Use low scores to refine agent instructions:

```
# Before (low relevancy scores)
"You are a helpful assistant."

# After (improved relevancy)
"You are a research assistant. When answering questions:
1. Directly address the user's question in the first sentence
2. Provide specific examples and evidence
3. Cite sources when making factual claims
4. If unsure, say so rather than guessing"
```

### 5. Monitor trends

Track scores over time to catch regressions:

```bash
curl "https://agentc2.ai/agent/api/agents/research-assistant/evaluations?groupBy=day&startDate=2026-02-01" \
  -H "Authorization: Bearer $TOKEN"
```

Look for:

- **Declining trends** — May indicate a regression
- **Improving trends** — Validate that changes are working
- **Seasonal patterns** — Understand usage variations

## Example scorer configuration

### Research agent

Focus on accuracy and completeness:

```json
{
    "scorers": ["relevancy", "completeness", "hallucination", "helpfulness"]
}
```

**Why:** Research agents need to be accurate (low hallucination), relevant, complete, and helpful.

### Support agent

Focus on helpfulness and tone:

```json
{
    "scorers": ["helpfulness", "tone", "relevancy"]
}
```

**Why:** Support agents need to be helpful, consistent in tone, and stay on topic.

### Public-facing agent

Focus on safety and brand:

```json
{
    "scorers": ["toxicity", "tone", "relevancy", "hallucination"]
}
```

**Why:** Public agents must be safe (low toxicity), consistent in tone, relevant, and factual.

## Custom scorers

While AgentC2 includes pre-built scorers, you can create custom scorers using Mastra's evaluation framework. Custom scorers can measure:

- **Domain-specific quality** — Industry-specific metrics
- **Business KPIs** — Conversion rates, user satisfaction proxies
- **Compliance** — Regulatory requirement checks

<Callout type="info">
    Custom scorers require code changes to the Mastra framework. Contact support for assistance
    implementing domain-specific evaluators.
</Callout>

## Best practices

### 1. Start with 2-3 scorers

Don't evaluate everything at once. Start with the most important dimensions:

```json
{
    "scorers": ["relevancy", "helpfulness"] // Start simple
}
```

Add more scorers as you identify specific quality issues.

### 2. Review low-scoring runs regularly

Set aside time each week to review runs with low scores. Look for patterns and iterate on instructions.

### 3. Use evaluations for A/B testing

Compare versions side-by-side using evaluation metrics. Promote the version with better scores.

### 4. Balance cost and quality

More scorers = higher compute costs. Balance evaluation coverage with cost:

- **High-stakes agents** — Evaluate with all relevant scorers
- **Low-stakes agents** — Use 1-2 key scorers
- **High-volume agents** — Consider batch evaluation

### 5. Set quality gates

Define minimum acceptable scores and block deployments that don't meet thresholds:

```bash
# Check if version meets quality gates
curl "https://agentc2.ai/agent/api/agents/research-assistant/evaluations?versionId=version-123&qualityGates=true" \
  -H "Authorization: Bearer $TOKEN"
```

## Related topics

- **[Guardrails](/docs/agents/guardrails)** — Prevent unsafe outputs (complements evaluations)
- **[Agent Learning](/docs/agents/learning)** — Use evaluations to improve agents over time
- **[Version Control](/docs/agents/version-control)** — Compare evaluation scores across versions
- **[Budgets and Costs](/docs/agents/budgets-and-costs)** — Balance evaluation costs with quality monitoring
