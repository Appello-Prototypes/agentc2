---
title: "Simulations"
description: "Test agents against simulated scenarios before production deployment."
section: "agents"
order: 12
primaryKeyword: "AI agent simulation"
relatedSlugs: ["agents/evaluations", "agents/learning"]
pageType: "how-to"
---

# Simulations

Simulations enable you to test agents against diverse, AI-generated scenarios before deploying changes to production. By running agents through hundreds or thousands of simulated conversations, you can identify edge cases, validate improvements, and compare performance across agent versions.

## When to Simulate

Simulations are valuable in several scenarios:

### Before Deploying New Versions

Test agent changes against a diverse set of scenarios before promoting to production:

```bash
# Create a version snapshot
curl -X POST https://agentc2.ai/agent/api/agents/support-agent/versions \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "changeNotes": "Added new tool for ticket creation"
  }'

# Run simulation to validate the change
curl -X POST https://agentc2.ai/agent/api/agents/support-agent/simulations \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "theme": "customer support scenarios",
    "count": 200,
    "concurrency": 10
  }'
```

### After Instruction Changes

Validate that instruction modifications improve behavior without introducing regressions:

```bash
# Update instructions
curl -X PATCH https://agentc2.ai/agent/api/agents/research-assistant \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "instructions": "You are a research assistant. Always cite sources and ask clarifying questions for ambiguous queries."
  }'

# Simulate to verify improvement
curl -X POST https://agentc2.ai/agent/api/agents/research-assistant/simulations \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "theme": "research queries with varying clarity",
    "count": 150
  }'
```

### Edge Case Testing

Discover how agents handle unusual or challenging inputs:

```bash
curl -X POST https://agentc2.ai/agent/api/agents/support-agent/simulations \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "theme": "angry customers, complex technical issues, refund requests, escalation scenarios",
    "count": 500,
    "concurrency": 20
  }'
```

<Callout type="info">
    Simulations use a simulator agent to generate diverse, realistic conversation prompts. The
    simulator agent understands your agent's capabilities and creates scenarios that test different
    aspects of behavior.
</Callout>

## Creating Simulation Scenarios

Simulations are defined by a **theme** that describes the types of scenarios to generate. The simulator agent uses this theme to create diverse, relevant conversation prompts.

### Theme Examples

| Theme                                     | Description                              | Use Case                       |
| ----------------------------------------- | ---------------------------------------- | ------------------------------ |
| `"customer support scenarios"`            | General support interactions             | Testing support agent behavior |
| `"research queries with varying clarity"` | Ambiguous to specific research questions | Testing query handling         |
| `"technical troubleshooting"`             | Complex technical problems               | Testing technical knowledge    |
| `"sales conversations"`                   | Sales interactions and objections        | Testing sales agent            |
| `"edge cases and adversarial inputs"`     | Unusual or challenging inputs            | Finding failure modes          |

### Starting a Simulation

Create a new simulation session:

```bash
curl -X POST https://agentc2.ai/agent/api/agents/research-assistant/simulations \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "theme": "research queries with varying clarity",
    "count": 100,
    "concurrency": 5
  }'
```

### Request Parameters

| Parameter     | Type   | Required | Default | Description                                  |
| ------------- | ------ | -------- | ------- | -------------------------------------------- |
| `theme`       | string | ✅ Yes   | —       | Theme describing scenarios to generate       |
| `count`       | number | ❌ No    | `100`   | Number of conversations to simulate (1-1000) |
| `concurrency` | number | ❌ No    | `5`     | Parallel conversations (1-10)                |

The API returns a simulation session:

```json
{
    "success": true,
    "session": {
        "id": "sim_abc123",
        "agentId": "cm_xyz789",
        "theme": "research queries with varying clarity",
        "status": "PENDING",
        "targetCount": 100,
        "concurrency": 5,
        "completedCount": 0,
        "failedCount": 0,
        "createdAt": "2026-02-18T12:00:00.000Z"
    }
}
```

## Running Simulations via API

Simulations run asynchronously via Inngest. After creating a session, the system:

1. **Generates diverse prompts** — The simulator agent creates conversation prompts matching your theme
2. **Executes conversations** — Runs each prompt through your agent, recording full execution traces
3. **Tracks progress** — Updates session status in real-time as conversations complete
4. **Collects metrics** — Aggregates scores, latency, costs, and success rates

### Monitoring Progress

Check simulation session status:

```bash
curl https://agentc2.ai/agent/api/agents/research-assistant/simulations/sim_abc123 \
  -H "Authorization: Bearer $TOKEN"
```

Response includes progress and aggregated metrics:

```json
{
    "success": true,
    "session": {
        "id": "sim_abc123",
        "status": "RUNNING",
        "targetCount": 100,
        "completedCount": 67,
        "failedCount": 2,
        "progress": 0.67,
        "metrics": {
            "avgQualityScore": 0.78,
            "avgSafetyScore": 0.94,
            "avgLatencyMs": 2150,
            "totalCostUsd": 2.34,
            "successRate": 0.97
        },
        "startedAt": "2026-02-18T12:00:00.000Z",
        "updatedAt": "2026-02-18T12:05:23.000Z"
    }
}
```

### Session Status

Simulation sessions progress through these statuses:

| Status      | Description                       |
| ----------- | --------------------------------- |
| `PENDING`   | Session created, waiting to start |
| `RUNNING`   | Actively executing conversations  |
| `COMPLETED` | All conversations finished        |
| `FAILED`    | Session failed with error         |

## Comparing Results Across Versions

Run simulations on multiple agent versions to compare performance:

### Baseline Version

```bash
# Run simulation on current production version
curl -X POST https://agentc2.ai/agent/api/agents/research-assistant/simulations \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "theme": "research queries",
    "count": 200
  }'
# Returns: sim_baseline_123
```

### Candidate Version

```bash
# Create new version with changes
curl -X PATCH https://agentc2.ai/agent/api/agents/research-assistant \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "instructions": "You are a research assistant. Always ask clarifying questions for ambiguous queries."
  }'

# Run simulation on candidate version
curl -X POST https://agentc2.ai/agent/api/agents/research-assistant/simulations \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "theme": "research queries",
    "count": 200
  }'
# Returns: sim_candidate_456
```

### Compare Results

Fetch both sessions and compare metrics:

```bash
# Get baseline results
curl https://agentc2.ai/agent/api/agents/research-assistant/simulations/sim_baseline_123 \
  -H "Authorization: Bearer $TOKEN"

# Get candidate results
curl https://agentc2.ai/agent/api/agents/research-assistant/simulations/sim_candidate_456 \
  -H "Authorization: Bearer $TOKEN"
```

Compare key metrics:

| Metric            | Baseline | Candidate | Improvement   |
| ----------------- | -------- | --------- | ------------- |
| Avg Quality Score | 0.72     | 0.79      | +0.07 (+9.7%) |
| Avg Safety Score  | 0.95     | 0.96      | +0.01 (+1.1%) |
| Success Rate      | 0.88     | 0.92      | +0.04 (+4.5%) |
| Avg Latency (ms)  | 2340     | 2280      | -60 (-2.6%)   |
| Total Cost (USD)  | 4.56     | 4.62      | +0.06 (+1.3%) |

<Callout type="tip">
    Use the same theme and count for both versions to ensure fair comparison. Consider running
    multiple simulation sessions and averaging results for statistical significance.
</Callout>

## Simulation Reporting

Each simulation session produces detailed reports including:

### Aggregated Metrics

- **Quality scores** — Average evaluation scores across all conversations
- **Safety scores** — Average safety evaluation scores
- **Success rate** — Percentage of conversations that completed successfully
- **Latency** — Average response time per conversation
- **Cost** — Total cost in USD (token usage, API calls)
- **Tool usage** — Frequency of each tool call
- **Error rate** — Percentage of conversations that failed

### Individual Run Traces

Every simulated conversation creates a full `AgentRun` with complete execution traces:

```bash
# List runs from a simulation session
curl "https://agentc2.ai/agent/api/agents/research-assistant/runs?sessionId=sim_abc123&limit=10" \
  -H "Authorization: Bearer $TOKEN"
```

Each run includes:

- Input prompt (generated by simulator)
- Agent response
- Tool calls and results
- Evaluation scores
- Token usage and cost
- Execution time
- Any errors or guardrail violations

### Exporting Results

Simulation results can be exported for analysis:

```bash
# Get all runs from simulation
curl "https://agentc2.ai/agent/api/agents/research-assistant/runs?sessionId=sim_abc123&limit=1000" \
  -H "Authorization: Bearer $TOKEN" > simulation_results.json
```

Analyze results programmatically:

```typescript
interface SimulationRun {
    id: string;
    input: string;
    output: string | null;
    qualityScore?: number;
    safetyScore?: number;
    latencyMs: number;
    costUsd: number;
    toolCalls: Array<{
        toolId: string;
        success: boolean;
    }>;
    status: "completed" | "failed";
}

// Analyze results
const runs: SimulationRun[] = await fetchRuns("sim_abc123");
const avgQuality = runs.reduce((sum, r) => sum + (r.qualityScore || 0), 0) / runs.length;
const successRate = runs.filter((r) => r.status === "completed").length / runs.length;
```

## Using Simulations with Evaluations

Simulations integrate with the evaluation system. When your agent has scorers configured, simulation runs automatically include evaluation scores:

### Configure Scorers

```bash
curl -X PATCH https://agentc2.ai/agent/api/agents/research-assistant \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "scorers": ["quality", "safety", "relevance"]
  }'
```

### Run Simulation with Evaluations

```bash
curl -X POST https://agentc2.ai/agent/api/agents/research-assistant/simulations \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "theme": "research queries",
    "count": 200
  }'
```

Each simulated conversation is automatically scored by all configured scorers. The simulation report includes:

- Average scores per scorer
- Score distributions
- Conversations with low scores (potential issues)
- Score trends across conversation order

### Analyzing Low-Score Conversations

Identify patterns in conversations that scored poorly:

```bash
# Get runs with quality score < 0.6
curl "https://agentc2.ai/agent/api/agents/research-assistant/runs?sessionId=sim_abc123&minQualityScore=0&maxQualityScore=0.6" \
  -H "Authorization: Bearer $TOKEN"
```

Review these conversations to understand failure modes and improve instructions or tools.

## Best Practices

1. **Start with smaller counts** — Begin with 50-100 conversations to validate your theme and agent behavior before scaling up
2. **Use diverse themes** — Test different aspects of agent behavior with varied themes
3. **Compare versions** — Always run simulations on both baseline and candidate versions for fair comparison
4. **Monitor costs** — Large simulations can be expensive; use `concurrency` to balance speed and cost
5. **Review low-score runs** — Focus on conversations with poor evaluation scores to identify improvement opportunities
6. **Use with learning system** — Combine simulations with continuous learning to validate proposed improvements before promotion

<Callout type="warning">
    Simulations create real agent runs that consume API credits and incur costs. Monitor your usage
    and set appropriate `count` and `concurrency` values based on your budget.
</Callout>

## Simulation Limitations

- **Not real user data** — Simulated conversations are AI-generated and may not reflect actual user behavior
- **Cost** — Large simulations consume API credits and incur costs
- **Time** — Simulations run asynchronously and can take time to complete (minutes to hours depending on count)
- **Theme dependency** — Results depend on how well the theme describes your use case

## Related Topics

- **[Evaluations](/docs/agents/evaluations)** — Learn how evaluation scorers measure agent performance
- **[Continuous Learning](/docs/agents/learning)** — Use simulations to validate learning system proposals
- **[Version Control](/docs/agents/version-control)** — Create versions before running simulations for comparison
