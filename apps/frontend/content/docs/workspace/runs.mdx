---
title: "Viewing Runs & History"
description: "View and understand agent run history in AgentC2. See what your agents did, review outputs, understand costs, and use history to improve performance."
section: "workspace"
order: 5
primaryKeyword: "AgentC2 run history"
relatedSlugs: ["workspace/chatting", "workspace/managing-agents", "workspace/overview"]
pageType: "how-to"
ctaLabel: "View Run History"
ctaHref: "/workspace"
---

# Viewing Runs & History

Every time you chat with an agent, AgentC2 creates a record of what happened. These records are called "runs," and they give you full visibility into what your agents are doing, how they're performing, and what they cost.

## What Is a Run?

A run is a complete record of a single agent interaction. It captures everything that happened during the conversation: what you asked, what the agent did behind the scenes, what tools it used, and what it answered. Think of it as a receipt for every piece of work your agent does.

## Where to Find Runs

You can access run history in two ways:

1. **From an agent's page** — Go to the **Agents** section, select an agent, and look for the **Runs** tab. This shows all runs for that specific agent.
2. **From the Runs section** — Click **Runs** in the sidebar to see runs across all agents in one place.

## What You'll See in a Run

Each run record includes:

| Field          | What It Shows                                                     |
| -------------- | ----------------------------------------------------------------- |
| **Timestamp**  | When the conversation happened                                    |
| **Input**      | The message you sent to the agent                                 |
| **Output**     | The agent's response                                              |
| **Tool Calls** | Which tools the agent used during the conversation                |
| **Model**      | Which AI model powered the response (e.g., GPT-4o, Claude Sonnet) |
| **Cost**       | How much the interaction cost in tokens and dollars               |
| **Status**     | Whether the run completed successfully or encountered an error    |

## Understanding Tool Calls

When an agent uses tools during a conversation (like searching HubSpot or checking a calendar), the run record shows each tool call in detail:

- **Which tool was used** — For example, "HubSpot: Search Contacts"
- **What was sent** — The query or parameters the agent passed to the tool
- **What came back** — The data the tool returned

This transparency helps you verify that the agent is using the right tools and getting accurate data. If something looks off in an agent's response, checking the tool calls is a great place to start investigating.

## Understanding Costs

Every agent interaction has a cost based on the AI model used and how much text was processed. The run details show:

- **Tokens used** — Tokens are the units AI models use to measure text. More text means more tokens.
- **Dollar cost** — The actual cost in dollars for that specific interaction

This information helps you understand your spending and make informed decisions about which models to use. If costs are higher than expected, consider switching to a lighter model for simpler tasks or tightening your agent's instructions to produce more concise responses.

## Filtering Runs

When you have a lot of run history, filters help you find what you're looking for:

- **By date** — See runs from a specific time period
- **By status** — Filter for successful runs, errors, or all
- **By agent** — Focus on runs from a particular agent

Use these filters to quickly narrow down to the runs you care about — whether you're troubleshooting an issue or reviewing recent activity.

## Evaluations and Quality Scores

If your team has enabled evaluations for an agent, you may see quality scores on runs. These are automated assessments of how well the agent performed:

- **Relevance** — Did the agent's answer address the question?
- **Accuracy** — Was the information correct?
- **Helpfulness** — Was the response actually useful?

Quality scores are shown as numbers or ratings next to each run. They're generated automatically and help your team spot trends — for example, if an agent's quality drops after a configuration change, you'll see it in the scores.

## Using Run History to Improve Agents

Run history is one of your best tools for making agents better. Here's how to use it:

1. **Find bad outputs** — Look for runs where the agent gave incorrect, unhelpful, or off-topic responses
2. **Understand why** — Check the tool calls and input to see what went wrong. Did the agent misunderstand the question? Use the wrong tool? Get bad data from a tool?
3. **Adjust instructions** — Update the agent's instructions to address the pattern. If the agent keeps making the same type of mistake, add clearer guidance about how to handle those situations
4. **Test again** — After making changes, test with similar questions to confirm the improvement
5. **Compare versions** — If you saved a version before your changes, compare run quality between the old and new versions

## Exporting Run Data

If you need run data for reporting or analysis outside of AgentC2:

1. Go to the Runs view
2. Apply any filters you need
3. Look for the **Export** option
4. Choose your preferred format

Exported data includes all the fields visible in the run details — timestamps, inputs, outputs, tool calls, costs, and quality scores.
