---
title: "Firecrawl"
description: "Connect agents to Firecrawl for web scraping, content extraction, and website crawling via Model Context Protocol."
section: "integrations"
order: 14
primaryKeyword: "Firecrawl web scraping"
relatedSlugs: ["integrations/overview", "integrations/model-context-protocol", "agents/tools"]
pageType: "how-to"
ctaLabel: "Explore Integrations in AgentC2"
ctaHref: "/workspace"
---

# Firecrawl

Firecrawl integration enables agents to scrape websites, extract structured content, and crawl entire sites for research and data collection. The integration uses Firecrawl's MCP server, providing standardized tool access to web scraping capabilities.

## Overview

The Firecrawl integration exposes web scraping operations as MCP tools that agents can call during conversations. Agents can scrape individual pages, crawl entire websites, extract structured data, and search web content—all while respecting crawl boundaries and rate limits.

<Callout type="info">
Firecrawl uses a self-hosted MCP server that communicates with Firecrawl's API. The integration requires an API key and handles authentication automatically.
</Callout>

## Prerequisites

- Firecrawl account with API access
- API key from Firecrawl dashboard
- Understanding of web scraping ethics and robots.txt

## Setup

### Step 1: Get Firecrawl API key

1. Log in to [Firecrawl Dashboard](https://firecrawl.dev)
2. Navigate to **API Keys**
3. Click **Create API Key** or copy existing key
4. Save the key securely (starts with `fc-`)

### Step 2: Configure in AgentC2

**Environment variables:**
```bash
FIRECRAWL_API_KEY="fc-..."
```

**Via UI:**
1. Navigate to **Settings > Integrations**
2. Find **Firecrawl** and click **Connect**
3. Enter your API key
4. Click **Test Connection** to verify credentials
5. Tools are automatically discovered and available

**Via API:**
```bash
curl -X POST https://agentc2.ai/agent/api/integrations/connections \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "providerKey": "firecrawl",
    "name": "Production Firecrawl",
    "credentials": {
      "FIRECRAWL_API_KEY": "fc-..."
    }
  }'
```

### Step 3: Attach tools to agents

List available Firecrawl tools:

```bash
curl https://agentc2.ai/agent/api/tools \
  -H "Authorization: Bearer $TOKEN" \
  | jq '.tools[] | select(.id | startswith("firecrawl-"))'
```

Attach tools to an agent:

```bash
curl -X POST https://agentc2.ai/agent/api/agents/research-assistant/tools \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "toolId": "firecrawl_firecrawl-scrape-url"
  }'
```

## Environment variables

| Variable | Required | Description |
|----------|----------|-------------|
| `FIRECRAWL_API_KEY` | Yes | Firecrawl API key (format: `fc-...`) |

<Callout type="warning">
Store `FIRECRAWL_API_KEY` securely. In production, use environment variables or encrypted credential storage. Never commit keys to version control.
</Callout>

## Available tools

| Tool Name | Description |
|-----------|-------------|
| `firecrawl_firecrawl-scrape-url` | Scrape a single URL and extract content |
| `firecrawl_firecrawl-crawl-url` | Crawl a website starting from a URL |
| `firecrawl_firecrawl-search` | Search the web using Firecrawl's search API |

Tool names follow the pattern `firecrawl_{tool-name}`. Use the tools API to see the full list with parameter schemas.

## Common patterns

### Scrape single page

```typescript
// Agent receives: "Scrape the content from example.com/article"
// Agent calls:
{
  tool: "firecrawl_firecrawl-scrape-url",
  arguments: {
    url: "https://example.com/article",
    formats: ["markdown", "html"] // Extract as markdown and HTML
  }
}
// Returns: Scraped content with markdown, HTML, metadata
```

### Crawl website

```typescript
// Agent receives: "Crawl example.com and extract all articles"
// Agent calls:
{
  tool: "firecrawl_firecrawl-crawl-url",
  arguments: {
    url: "https://example.com",
    limit: 10, // Max pages to crawl
    includePaths: ["/articles/"], // Only crawl articles
    excludePaths: ["/admin/", "/private/"] // Exclude paths
  }
}
// Returns: Array of scraped pages with content
```

### Search web

```typescript
// Agent receives: "Search for information about AI agents"
// Agent calls:
{
  tool: "firecrawl_firecrawl-search",
  arguments: {
    query: "AI agents framework",
    limit: 5
  }
}
// Returns: Search results with URLs and snippets
```

## Crawl boundaries

### Include/exclude paths

Control which pages are crawled:

```typescript
{
  includePaths: ["/blog/", "/articles/"], // Only these paths
  excludePaths: ["/admin/", "/private/", "/api/"] // Exclude these
}
```

### Limit depth

Control crawl depth:

```typescript
{
  maxDepth: 2, // Only crawl 2 levels deep
  limit: 50 // Max 50 pages total
}
```

### Respect robots.txt

Firecrawl respects `robots.txt` by default:

```typescript
{
  respectRobotsTxt: true // Default: true
}
```

<Callout type="tip">
Always respect `robots.txt` and website terms of service. Use crawl boundaries to limit scope and avoid overloading servers.
</Callout>

## Content extraction

### Formats

Firecrawl can extract content in multiple formats:

- **`markdown`**: Clean markdown text
- **`html`**: Original HTML
- **`rawHtml`**: Raw HTML with scripts/styles
- **`links`**: Extracted links
- **`screenshot`**: Page screenshot (if available)

```typescript
{
  formats: ["markdown", "html", "links"]
}
```

### Structured data

Extract structured data using selectors:

```typescript
{
  extract: {
    title: { selector: "h1" },
    author: { selector: ".author" },
    content: { selector: ".article-content" }
  }
}
```

## Freshness and caching

### Cache control

Control caching behavior:

```typescript
{
  cache: false // Don't use cached content
}
```

### Freshness

Request fresh content:

```typescript
{
  freshness: "always" // Always fetch fresh content
}
```

<Callout type="info">
Firecrawl caches content to reduce API calls and improve performance. Use `cache: false` only when you need absolutely fresh content.
</Callout>

## Troubleshooting

| Problem | Solution |
|---------|----------|
| **"Authentication failed"** | Verify `FIRECRAWL_API_KEY` is correct and not expired. Regenerate key in Firecrawl dashboard if needed. |
| **"URL not accessible"** | Check URL is publicly accessible (not behind authentication). Verify URL format is correct. |
| **"Rate limit exceeded"** | Firecrawl limits API calls per plan. Upgrade plan or implement rate limiting/queuing. |
| **"Crawl timeout"** | Large crawls may timeout. Reduce `limit` or `maxDepth`, or use `scrape-url` for individual pages. |
| **"Content extraction failed"** | Some pages may block scraping. Check if site allows scraping (robots.txt, terms of service). |
| **"Invalid selector"** | Verify CSS selectors are correct. Test selectors in browser DevTools before using in extraction. |

## Rate limiting

Firecrawl API rate limits vary by plan:

- **Free tier**: Limited requests/month
- **Starter**: Higher limits
- **Pro/Enterprise**: Higher limits + priority

Monitor usage:

```bash
curl https://api.firecrawl.dev/v1/usage \
  -H "Authorization: Bearer $FIRECRAWL_API_KEY"
```

## Best practices

### 1. Respect crawl boundaries

Always set appropriate limits:

```typescript
{
  limit: 10, // Don't crawl entire sites
  maxDepth: 2, // Limit depth
  includePaths: ["/relevant/"] // Only relevant paths
}
```

### 2. Use single-page scraping when possible

For individual pages, use `scrape-url` instead of `crawl-url`:

```typescript
// Better: Single page
{
  tool: "firecrawl_firecrawl-scrape-url",
  arguments: { url: "https://example.com/article" }
}

// Avoid: Full crawl for single page
{
  tool: "firecrawl_firecrawl-crawl-url",
  arguments: { url: "https://example.com/article", limit: 1 }
}
```

### 3. Handle errors gracefully

Some sites may block scraping:

```typescript
try {
  const result = await callTool("firecrawl_firecrawl-scrape-url", { url });
  return result.content;
} catch (error) {
  if (error.message.includes("blocked") || error.message.includes("403")) {
    return "This website blocks automated scraping. Please provide the content manually.";
  }
  throw error;
}
```

### 4. Cache results

Cache scraped content to reduce API calls:

```typescript
const cacheKey = `firecrawl:${url}`;
const cached = await cache.get(cacheKey);
if (cached) return cached;

const result = await callTool("firecrawl_firecrawl-scrape-url", { url });
await cache.set(cacheKey, result, 3600); // Cache 1 hour
```

## Ethical considerations

### Respect robots.txt

Always respect `robots.txt`:

```typescript
{
  respectRobotsTxt: true // Default and recommended
}
```

### Rate limiting

Don't overload servers:

```typescript
// Add delays between requests
await sleep(1000); // 1 second delay
```

### Terms of service

Check website terms before scraping:
- Some sites prohibit scraping
- Some require attribution
- Some have API alternatives

<Callout type="warning">
Always review website terms of service and robots.txt before scraping. Use official APIs when available instead of scraping.
</Callout>

## Related topics

- **[Integrations Overview](/docs/integrations/overview)** — Learn about integration architecture
- **[Model Context Protocol](/docs/integrations/model-context-protocol)** — Understand MCP tool execution
- **[Agent Tools](/docs/agents/tools)** — Attach tools to agents